{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download USGS Data \n",
    "This script is to download USGS data from both the nwis package and the website. Downloading from NWIS is faster and efficient compared to from the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataretrieval import nwis\n",
    "import logging\n",
    "\n",
    "# Setup basic configuration for logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load the gauge file\n",
    "gauge_file = pd.read_excel('usgs_with_comid.xlsx')\n",
    "gauge_file = gauge_file[0:5]\n",
    "\n",
    "# Ensure site numbers are strings of length 8 with leading zeros if necessary\n",
    "gauge_file['station_no'] = gauge_file['station_no'].apply(lambda x: f\"{x:08d}\")\n",
    "\n",
    "# Extract the correctly formatted site numbers\n",
    "sites = gauge_file['station_no']\n",
    "\n",
    "# Define dates and parameter code\n",
    "start, end = \"2000-01-01\", \"2024-07-01\"\n",
    "parameter_code = '00060'  # Discharge, cubic feet per second\n",
    "\n",
    "# Create an empty DataFrame to store combined data\n",
    "combined_discharge = pd.DataFrame()\n",
    "\n",
    "# Process each site\n",
    "for site in sites:\n",
    "    try:\n",
    "        logging.info(f\"Fetching data for site {site}\")\n",
    "        discharge = nwis.get_record(sites=site, service='iv', parameterCd=parameter_code, start=start, end=end)\n",
    "\n",
    "        if not discharge.empty:\n",
    "            discharge_columns = [col for col in discharge.columns if '00060' in col and 'cd' not in col]\n",
    "            if discharge_columns:\n",
    "                discharge_column = discharge_columns[0]\n",
    "                discharge[site] = discharge[discharge_column] * 0.0283168  # Convert cfs to cms\n",
    "                discharge.index = pd.to_datetime(discharge.index)\n",
    "                hourly_discharge = discharge[[site]].resample('h').mean()\n",
    "\n",
    "                if combined_discharge.empty:\n",
    "                    combined_discharge = hourly_discharge\n",
    "                else:\n",
    "                    combined_discharge = pd.concat([combined_discharge, hourly_discharge], axis=1)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process data for site {site}: {e}\")\n",
    "\n",
    "combined_discharge.reset_index(inplace=True)\n",
    "combined_discharge.rename(columns={'index': 'datetime'}, inplace=True)\n",
    "combined_discharge['datetime'] = pd.to_datetime(combined_discharge['datetime']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Save and log\n",
    "combined_discharge.to_csv('usgs_data.csv', index=False)\n",
    "logging.info(\"Combined discharge data saved to 'discharge_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website: This script was written by passaH2O group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from time import sleep\n",
    "from dask import delayed, compute\n",
    "from tqdm import tqdm\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Load COMID and gauge information\n",
    "df = pd.read_excel('usgs_with_comid.xlsx')\n",
    "df = df[0:5]  # Limit to the first 5 gauges for testing\n",
    "comid = df.COMID.values\n",
    "gage_num = df.station_no.values\n",
    "gage_name = df.station_nm.values\n",
    "\n",
    "# Example outputs to check\n",
    "print(f'Example COMID: {comid[0]}')\n",
    "print(f'Example Gauge ID: {gage_num[0]}')\n",
    "\n",
    "# Define start and end dates for data retrieval\n",
    "START_DATE = '1979-02-01'\n",
    "END_DATE = '2023-02-01'\n",
    "\n",
    "# Define the folder where CSV files will be saved\n",
    "folder_name = 'usgs_1979_2023_website'\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Function to parse the datetime in the JSON response\n",
    "def row_time(row):\n",
    "    time_split = row.Time.split('T')\n",
    "    date = time_split[0]\n",
    "    time = time_split[1].split('.')[0]\n",
    "    return date, time\n",
    "\n",
    "# Function to fetch data from the USGS API and convert it to a DataFrame\n",
    "def url_to_csv(url):\n",
    "    try:\n",
    "        # Convert requested URL to JSON\n",
    "        jso = requests.get(url).json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed for {url}: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    # Subset the time series data\n",
    "    dict_temp_a = list(jso.values())[3]['timeSeries']\n",
    "    time_series_length = len(dict_temp_a)\n",
    "    \n",
    "    if time_series_length > 1:\n",
    "        variableTypes = []\n",
    "        dict_temp = []\n",
    "        dict_temp2 = []\n",
    "\n",
    "        # Loop through the available data\n",
    "        for NUM, aaa in enumerate(dict_temp_a):\n",
    "            variableNameVal = aaa['variable']['variableCode'][0]['value']\n",
    "            variableTypes.append(variableNameVal)\n",
    "\n",
    "            # Discharge data\n",
    "            if variableNameVal == '00060':\n",
    "                dict_temp = dict_temp_a[NUM]['values'][0]['value']\n",
    "\n",
    "            # Stage data\n",
    "            elif variableNameVal == '00065':\n",
    "                dict_temp2 = dict_temp_a[NUM]['values'][0]['value']\n",
    "\n",
    "        # Check if both discharge and stage data are available\n",
    "        if '00065' in variableTypes:\n",
    "            times = []\n",
    "            discharge = []\n",
    "            stage = []\n",
    "            times2 = []\n",
    "\n",
    "            # Collect discharge and stage data\n",
    "            for i, vals in enumerate(dict_temp):\n",
    "                times.append(vals['dateTime'])\n",
    "                discharge.append(vals['value'])\n",
    "            for j, vals2 in enumerate(dict_temp2):\n",
    "                times2.append(vals2['dateTime'])\n",
    "                stage.append(vals2['value'])\n",
    "\n",
    "            # Create DataFrames\n",
    "            value_dict = {\"Time\": times, \"Discharge\": discharge}\n",
    "            value_dict2 = {\"Time\": times2, \"Stage\": stage}\n",
    "            df1 = pd.DataFrame(value_dict)\n",
    "            df2 = pd.DataFrame(value_dict2)\n",
    "\n",
    "            # Merge stage and discharge data\n",
    "            df = pd.merge(df1, df2, how='left', left_on='Time', right_on='Time')\n",
    "\n",
    "            # Process the time information\n",
    "            dates = df.apply(row_time, axis=1)\n",
    "            date = []\n",
    "            measure_time = []\n",
    "            for j in dates:\n",
    "                date.append(j[0])\n",
    "                measure_time.append(j[1])\n",
    "\n",
    "            df.drop(columns=[\"Time\"], inplace=True)\n",
    "            df['Date'] = date\n",
    "            df['Time_temp'] = measure_time\n",
    "            df['Time'] = pd.to_datetime(df['Date'] + ' ' + df['Time_temp'])\n",
    "            df.drop(columns=[\"Date\", \"Time_temp\"], inplace=True)\n",
    "            return df\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to process each gage in parallel\n",
    "@delayed\n",
    "def process_gage(gage_num, comid, folder_name, START_DATE, END_DATE):\n",
    "    url = f'https://waterservices.usgs.gov/nwis/iv/?sites=0{gage_num}&startDT={START_DATE}&endDT={END_DATE}&format=json'\n",
    "    print(f\"Fetching data from {url}\")\n",
    "    df = url_to_csv(url)\n",
    "\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df['Gage'] = gage_num\n",
    "        df['Comid'] = comid\n",
    "        df['URL_data'] = url\n",
    "        # Save the file to the designated folder\n",
    "        df.to_csv(f'{folder_name}/discharge_0{gage_num}.csv')\n",
    "        sleep(0.75)  # To avoid overwhelming the server\n",
    "        return f\"Saved data for Gauge: {gage_num}\"\n",
    "    else:\n",
    "        sleep(0.2)\n",
    "        return f\"No data for Gauge: {gage_num}\"\n",
    "\n",
    "# Process all gauges\n",
    "def fetch_all_gages(gage_num, comid, folder_name, START_DATE, END_DATE):\n",
    "    tasks = []\n",
    "    for i in range(len(gage_num)):\n",
    "        tasks.append(process_gage(gage_num[i], comid[i], folder_name, START_DATE, END_DATE))\n",
    "\n",
    "    # Use Dask to execute in parallel with progress tracking\n",
    "    with ProgressBar():\n",
    "        results = compute(*tasks)\n",
    "    \n",
    "    # Print final results\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "# Run the main function to fetch data for all gauges\n",
    "fetch_all_gages(gage_num, comid, folder_name, START_DATE, END_DATE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('assimilation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bc09726d3eb0feb4d73d3636dad7388c6ae5ba645f208395a639ba59e5d5f59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
